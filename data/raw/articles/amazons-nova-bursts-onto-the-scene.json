{
  "title": "Amazon’s Nova bursts onto the scene",
  "url": "https://www.deeplearning.ai/the-batch/amazons-nova-bursts-onto-the-scene/",
  "text": "Twice a week, Data Points brings you the latest AI news, tools, models, and research in brief. In today’s edition, you’ll find:\nHunyuanVideo vies with Sora, etc.\nA new record for underwater robot speed\nApple partners with Amazon for search and inference\nMore publishers sign on with Perplexity\nBut first:\nAmazon’s Nova promises lower costs and multimodal performance\nAmazon introduced its new Nova line of AI models, including Micro (text-only), Lite (multimodal), Pro (advanced multimodal), Premier (complex reasoning), Canvas (image generation), and Reel (video generation). The company claims Nova models are at least 75% less expensive than comparable models on Amazon Bedrock and offer the fastest performance in their respective intelligence classes. Nova models support 200 languages, custom fine-tuning, and integration with Amazon Bedrock Knowledge Bases for improved RAG accuracy. ( Amazon )\nOpenAI launches more capable o1 model with visual reasoning\nOpenAI made o1, an upgraded version of its o1-preview model with enhanced reasoning and coding abilities, available for paid users of ChatGPT. The updated model features faster processing, more concise thinking, and can read image inputs, enabling visual as well as textual reasoning. OpenAI reports o1 reduces major errors on difficult real-world questions by 34 percent compared to o1-preview and released a system card with a set of safety evaluations. ( OpenAI )\nTencent unveils open-source video generator that rivals top models\nTencent released HunyuanVideo, an open-source video generation AI that performs comparably to leading closed-source models. The 13-billion-parameter model uses unusual techniques like joint image-video training and a custom 3D architecture. According to Tencent HunyuanVideo outperforms models from Runway, Luma, and other top Chinese companies on human evaluations of visual quality and text alignment. The release of HunyuanVideo’s code and weights could narrow the gap between proprietary and open-source video AI capabilities. ( GitHub )\nManta ray-inspired soft robot sets new underwater speed record\nA team at North Carolina State University created a soft robot that swims at 6.8 body lengths per second, nearly doubling their previous record. The robot features manta ray-inspired fins attached to a flexible body with an air chamber, allowing it to swim on the surface and underwater by mimicking manta ray movements. This advancement in soft robotics demonstrates improved speed, energy efficiency, and maneuverability, paving the way for potential applications in underwater exploration and payload transportation. ( North Carolina State University )\nApple taps Amazon chips for search and potential model training\nApple confirmed it uses Amazon Web Services’ custom AI chips for consumer search queries, achieving 40 percent greater efficiency. The company is also evaluating Amazon’s Trainium2 chip for pre-training AI models, expecting up to 50 percent improvement in efficiency. This collaboration between tech giants shows that even Apple, known for its in-house approach, recognizes the value of specialized AI hardware in pushing the boundaries of what’s possible in AI development. ( AppleInsider )\nPerplexity adds global media partners to enrich AI search results\nPerplexity welcomed over a dozen new partners to its Publishers’ Program, including the Los Angeles Times , The Independent , and other media brands from the UK, Japan, Spain, and Latin America. The new partners cover a wide range of topics, from specialized trade coverage to local reporting, and will share in revenue generated from advertising while gaining access to Perplexity’s APIs and developer support. This global expansion not only enriches Perplexity’s knowledge base but also could make AI-powered search more worldly and insightful. ( Perplexity )\nStill want to know more about what matters in AI right now?\nRead this week’s issue of The Batch for in-depth analysis of news and research.\nThis week, Andrew Ng debunked the idea that building with generative AI is costly, explaining that while training foundation models is expensive, prototyping and creating applications using existing tools is now very affordable, with costs as low as a few dollars.\n“Because of the massive investments in foundation models, it’s now incredibly inexpensive to experiment and build prototypes in the applications layer! Over Thanksgiving holiday, I spent about one and a half days prototyping different generative AI applications, and my bill for OpenAI API calls came out to about $3.”\nRead Andrew’s full letter here .\nOther top AI news and research stories we covered in depth: Stripe introduced an ecommerce agent toolkit enabling AI to securely spend money; Mistral launched Pixtral Large , a strong competitor in vision-language models; the generative AI and GPU boom is raising concerns over increasing e-waste ; and a research paper explored the E-DPO method which enhances defenses against jailbreak prompts , reinforcing AI security.\nSubscribe to Data Points\n\n\n",
  "image_filename": "amazons-nova-bursts-onto-the-scene.png"
}
{
  "title": "DeepSeek’s R1 seeks to match OpenAI’s o1",
  "url": "https://www.deeplearning.ai/the-batch/deepseeks-r1-seeks-to-match-openais-o1/",
  "text": "Twice a week, Data Points brings you the latest AI news, tools, models, and research in brief. In today’s edition, you’ll find:\nFlux’s new image tools compete with Adobe and other photo apps\nQwen speeds up its 2.5 model while boosting context window\nRabbit launches Teach Mode in beta for its R1 devices\nBanks embrace third-party AI, but want to control their data\nBut first:\nDeepSeek challenges OpenAI with new open reasoning model\nDeepSeek, an AI research company, released a preview of DeepSeek-R1, a reasoning model that claims to match the performance of OpenAI’s o1 on key math and coding benchmarks. The model spends more time considering questions to improve accuracy, potentially taking tens of seconds to respond depending on the complexity of the task. For now, DeepSeek is making a preview version of its model available for a limited number of inquiries, but the company plans to make an open-source version available soon. ( DeepSeek )\nMicrosoft strikes deal with HarperCollins to train on its book catalog\nMicrosoft reached an agreement with HarperCollins to use select nonfiction books for training an unannounced AI model. The deal allows limited use of backlist titles, with authors given the option to participate, and includes safeguards to protect authors’ rights and revenue streams. This agreement highlights the growing trend of tech companies seeking high-quality, licensed content to improve their AI models’ performance and expertise in specific subjects. ( Bloomberg and The Verge )\nFlux expands AI image editing capabilities with new tool suite\nBlack Forest Labs unveiled FLUX.1 Tools, a suite of AI models designed to enhance control and editing capabilities for its text-to-image model FLUX.1. The suite includes four features: Fill for inpainting and outpainting, Depth and Canny for structural guidance, and Redux for image variation and restyling. Flux is offering these tools as open-access models for researchers and through its API for commercial use, demonstrating its commitment to both the research community and industry applications. ( Black Forest Labs )\nAlibaba releases Qwen2.5-Turbo with million-token context window\nAlibaba extended Qwen2.5-Turbo’s context length from 128,000 to 1 million tokens, enabling it to process about 10 full-length novels or 30,000 lines of code at once. The model outperforms GPT-4 on long-text evaluation benchmarks while maintaining competitive performance on shorter sequences. Qwen2.5-Turbo’s improvements in speed and cost-effectiveness make it a competitive alternative for AI developers. ( GitHub )\nRabbit’s AI agent learns to automate tasks from user demonstrations\nRabbit released a Teach Mode beta for all R1 users, allowing them to instruct the device’s AI agent to perform complex tasks across various platforms. The feature, part of Rabbit’s Large Action Model (LAM) system, learns from user demonstrations and can adapt to similar tasks, aiming to simplify human-computer interaction by making app interfaces invisible to users. Rabbit seeks to build an AI-native operating system to replace traditional app-based ecosystems, but early versions of the R1’s software have not delivered that promise. ( Rabbit )\nFinancial firms embrace AI despite data-related concerns\nA new Bank of England survey reveals 75 percent of financial firms already use AI, with an additional 10 percent planning adoption within three years. Foundation models now account for 17 percent of all AI use cases, while third-party implementations have risen to 33 percent of use cases. Data-related issues top the list of perceived AI risks, but firms expect benefits to outpace risks over the next three years. ( Bank of England )\nStill want to know more about what matters in AI right now?\nRead this week’s issue of The Batch for in-depth analysis of news and research.\nThis week, Andrew Ng explored an emerging trend of writing text to be read specifically by AI models, discussing how it parallels SEO and how incentives might drive authors to create content tailored for LLM consumption.\n“A small number of people are posting text online that’s intended for direct consumption not by humans, but by LLMs (large language models). I find this a fascinating trend, particularly when writers are incentivized to help LLM providers better serve their users!”\nRead Andrew’s full letter here .\nOther top AI news and research stories we covered in depth: Next-gen models show limited gains as AI giants rethink their training strategies amidst the breakdown of scaling laws; AI creates an interactive Minecraft-like world in real time, eliminating the need for a game engine; TSMC halts advanced chip production for Chinese companies following new U.S. orders, escalating chip restrictions; and researchers achieve a 20 percent reduction in transformer training costs with minimal performance loss, paving the way for more efficient AI development.\nSubscribe to Data Points\n\n\n",
  "image_filename": "deepseeks-r1-seeks-to-match-openais-o1.jpg"
}
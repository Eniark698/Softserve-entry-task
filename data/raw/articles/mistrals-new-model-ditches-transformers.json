{
  "title": "Mistral’s new model ditches transformers",
  "url": "https://www.deeplearning.ai/the-batch/mistrals-new-model-ditches-transformers/",
  "text": "Twice a week, Data Points brings you the latest AI news, tools, models, and research in brief. In today’s edition, you’ll find:\nGroq’s new tool use models\nAnother AI safety and security industry group\nMicrosoft’s new research applying LLMs to spreadsheets\nProposed safeguards to protect open AI models\nBut first:\nMistral releases open-source Mamba-based code model Mistral AI released Codestral Mamba 7B, a new 7 billion parameter language model specializing in code generation. As the name suggests, Codestral Mamba is based on the Mamba2 architecture rather than the usual transformer architecture. The model offers linear time inference, can handle sequences of infinite length, and performs on par with state-of-the-art Transformer-based models in advanced code and reasoning tasks. Codestral 22B and Codestral Mamba 7B outperform other coding models in their size classification, including CodeGemma, CodeLlama, and DeepSeek Coder. Codestral Mamba 7B’s release under the Apache 2.0 license, along with its flexible deployment options, positions it as a significant tool for developers and researchers in AI architecture and coding technology. ( Hugging Face )\nAI speeds development of new herbicides to combat resistant weeds Major agriculture companies are using artificial intelligence to accelerate the development of new herbicides and pesticides. Bayer’s AI system “CropKey” helped create Icafolin, a new weed-killing chemical set for release in Brazil in 2028, which the company claims will be the first wholly novel herbicide mode of action in over 30 years. This AI-driven approach could reduce the time to bring new products to market from 15 years to 10 years, according to Syngenta. The push for AI-assisted chemical development comes as farmers struggle with weeds that have become resistant to multiple herbicides, threatening the entire agriculture industry. ( The Wall Street Journal and Bayer )\nGroq’s new Llama 3 models specialize in tool use Groq unveiled two new open models, Llama-3-Groq-70B-Tool-Use and Llama-3-Groq-8B-Tool-Use, designed specifically for tool use and function calling. The models are best used in a hybrid approach with a general-purpose language model, where queries are routed to each model depending on which would best handle a given request. Both models are now available on GroqCloud Developer Hub and Hugging Face, released under the same license as the original Llama-3 models. The 70 billion parameter model outperforms all other open-source and proprietary models on the Berkeley Function Calling Leaderboard, achieving 90.76% overall accuracy. ( Groq )\nTech giants unite to develop shared AI security standards The Coalition for Secure AI (CoSAI) was announced at the Aspen Security Forum, bringing together industry leaders, academics, and experts to create open-source guidance and tools for developing secure AI systems. CoSAI’s initial work will focus on three key areas: enhancing software supply chain security for AI systems, preparing defenders for AI-related cybersecurity challenges, and developing AI security governance best practices and risk assessment frameworks. With founding sponsors including Google, IBM, Microsoft, Amazon, and OpenAI, this initiative marks a significant industry-wide effort to establish comprehensive security measures that address both classical and unique risks associated with AI. ( Oasis )\nNew approach overcomes LLMs’ token constraints when interpreting spreadsheets Microsoft researchers developed SpreadsheetLLM, a system that helps AI models better understand and work with spreadsheets. The system uses a new encoding method called SheetCompressor, which outperforms existing models by over 12 percent in detecting spreadsheet tables and achieves a 25-times compression ratio. This advancement could significantly improve AI’s ability to analyze complex spreadsheet information, potentially transforming how businesses and researchers work with tabular data. ( arXiv )\nExperts propose strategies to govern open AI models responsibly A workshop hosted by GitHub and Partnership on AI explored safeguards for open foundation models, recommending a series of risk mitigation strategies across the AI value chain. Key recommendations include implementing disclosure mechanisms for generated content, conducting safety evaluations, and establishing incident response policies. The experts stress the importance of understanding the complex AI ecosystem to craft effective governance, suggesting that different actors like model providers, adapters, and application developers all have roles in preventing misuse and ensuring responsible AI development. ( PAI )\nStill want to know more about what matters in AI right now?\nRead last week’s issue of The Batch for in-depth analysis of news and research.\nLast week, Andrew Ng wrote discussed political violence and AI’s role in strengthening democracy:\n“Looking into the future, in addition to specific applications that strengthen elements of democracy, I hope we keep on promoting widespread access to technology. This will enhance fairness and the ability of individuals to vote wisely. That’s why democratizing access to technology will help democracy itself.”\nRead Andrew’s full letter here .\nOther top AI news and research stories we covered in depth: Copyright claim fails in GitHub case , a paper that ranks popular models for openness , an arena-style contest that pits the world’s best text-to-image generators against each other , and a new way to identify hallucinations .\nSubscribe to Data Points\n\n\n",
  "image_filename": "mistrals-new-model-ditches-transformers.jpg"
}
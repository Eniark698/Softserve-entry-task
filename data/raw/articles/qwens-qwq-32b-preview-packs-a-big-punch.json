{
  "title": "Qwen’s QwQ-32B-Preview packs a big punch",
  "url": "https://www.deeplearning.ai/the-batch/qwens-qwq-32b-preview-packs-a-big-punch/",
  "text": "Twice a week, Data Points brings you the latest AI news, tools, models, and research in brief. In today’s edition, you’ll find:\nJina updates CLIP embedding model\nOLMo2 delivers state-of-the-art performance for smaller open models\nNvidia’s Fugatto produces a wide range of sounds\nU.S. antitrust regulator takes a hard look at Microsoft\nBut first:\nAlibaba’s experimental open-weight model shows promise in math and coding tasks\nThe Qwen team has released QwQ-32B-Preview, an experimental o1-like language model focused on enhancing AI reasoning capabilities. The model demonstrates impressive performance on challenging math and programming benchmarks, achieving scores of 65 percent on GPQA, 50 percent on AIME, over 90 percent on MATH-500, and 50 percent on LiveCodeBench. Despite its strengths, QwQ-32B-Preview has limitations including language mixing, recursive reasoning loops, and safety concerns. Still, the model’s reasoning power at just 32 billion parameters is a noteworthy achievement. ( GitHub )\nNew open standard aims to improve AI assistants’ data access\nAnthropic unveiled the Model Context Protocol (MCP), an open standard for connecting AI assistants to various data sources. MCP aims to replace fragmented integrations with a universal protocol, allowing AI systems to access relevant data more easily from content repositories, business tools, and development environments. This release includes the MCP specification, SDKs, local server support in Claude Desktop apps, and an open-source repository of pre-built servers for popular enterprise systems. ( Anthropic )\nJina AI’s new model boosts multilingual multimodal embeddings\nJina AI unveiled Jina-CLIP v2, a 0.9 billion parameter model that supports 89 languages and processes images at 512x512 resolution. The model outperforms its predecessor on cross-modal retrieval tasks and matches state-of-the-art performance on several benchmarks. This release aims to enhance multimodal search and retrieval capabilities for developers globally, breaking down language barriers in AI applications. ( Jina )\nAi2 releases OLMo 2, a new family of open language models\nAi2 introduced OLMo 2-7B and OLMo 2-13B, a family of models trained on up to 5 trillion tokens. The models achieve performance on par with or better than equivalently sized fully open models and are competitive with open-weight models like Llama 3.1 on English academic benchmarks. AI2 focused on improving training stability, implementing staged training interventions, and developing state-of-the-art post-training recipes to create OLMo 2-Instruct models. ( Ai2 )\nNvidia AI researchers unveil versatile audio generation model\nResearchers at Nvidia created Fugatto, an AI model that can generate or transform any mix of music, voices, and sounds using text prompts and audio files. The model allows users to modify existing audio, create entirely new sounds, and combine instructions in novel ways, giving fine-grained control over attributes like accents, emotions, and temporal changes. Fugatto’s capabilities have potential applications in music production, advertising, language learning, and game development. ( Nvidia )\nFTC scrutinizes Microsoft’s market power in cloud and AI\nThe U.S. Federal Trade Commission reportedly opened an investigation into Microsoft’s potential antitrust violations across multiple business segments. The agency is examining Microsoft’s cloud computing, AI, and cybersecurity products, with a focus on how the company bundles its offerings and its growing influence in AI. This investigation continues the U.S. government’s efforts to scrutinize major tech companies, though the regulatory landscape may shift with the upcoming change in administration. ( The New York Times )\nStill want to know more about what matters in AI right now?\nRead this week’s issue of The Batch for in-depth analysis of news and research.\nThis week, Andrew Ng shared his gratitude for Thanksgiving, reflected on the struggles of those less fortunate, and emphasized the importance of understanding diverse perspectives to create impactful technology. He highlighted his optimism about AI’s potential to improve lives and encouraged the community to keep building solutions to help others.\n“Technology remains the best way I know of to help people at scale through providing better education, career guidance, healthcare, personal safety, healthier food, or other things needed to support thriving.”\nRead Andrew’s full letter here .\nOther top AI news and research stories we covered in depth: DeepSeek-R1 challenges OpenAI o1 with a transparent model revealing its reasoning ; π0 advances household robotics with an innovative machine learning system; Amazon deepens its partnership with Anthropic through a $4 billion investment; and Grounding DINO 1.5 enhances object detection on small devices with faster and smarter capabilities.\nSubscribe to Data Points\n\n\n",
  "image_filename": "qwens-qwq-32b-preview-packs-a-big-punch.jpg"
}
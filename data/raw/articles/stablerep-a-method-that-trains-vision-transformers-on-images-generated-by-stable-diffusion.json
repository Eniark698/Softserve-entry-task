{
  "title": "Synthetic Data Helps Image Classification",
  "url": "https://www.deeplearning.ai/the-batch/stablerep-a-method-that-trains-vision-transformers-on-images-generated-by-stable-diffusion/",
  "text": "Generated images can be more effective than real ones in training a vision model to classify images.\nWhat's new: Yonglong Tian, Lijie Fan, and colleagues at Google and MIT introduced StableRep , a self-supervised method that trains vision transformers on images generated by Stability.AI’s Stable Diffusion image generator.\nKey insight: Models that employ a contrastive loss learn to represent examples as more or less similar. For example, images that depict a particular object are more similar to each other, and images that depict other objects are less similar to the first group. The training method known as SimCLR uses a contrastive loss with two augmented (cropped, rotated, flipped, and so on) versions of each image, so a model learns that augmented versions of one image, which is closely related but different, are similar to one another — but not to augmented versions of other images. Given a prompt, an image generator produces images that are closely related but significantly more different than augmented versions of the same image. This makes for greater variety among similar examples, which can lead to more effective learning using a contrastive loss.\nHow it works: The authors generated images and trained a vision transformer on them using a contrastive loss.\nThe authors used Stable Diffusion to generate 2.7 million images. They drew the prompts from the captions in Conceptual Captions (a dataset of images and captions) and asked Stable Diffusion to generate 10 images of each prompt.\nThey augmented each generated image according to SimCLR, but only once.\nThey trained a ViT-B/16 to generate a similar embedding for the augmented version of each image generated from the same prompt, and a dissimilar embedding for the augmented version of each image generated from other prompts.\nResults: The authors compared the ViT-B/16 trained using StableRep to two models of the same architecture trained using SimCLR (one using generated images, the other using images from Conceptual Captions). They also compared it to two CLIP models that produced matching embeddings for images and their paired captions, one trained on generated images and their prompts, the other on real images and their captions. For each of 11 computer vision datasets, the authors trained a linear classifier on top of each model without changing the model’s weights. Comparing the classifiers’ performance, StableRep achieved the best results on 9 of them. For example, on FGVC-Aircraft (10,000 images of 100 different aircraft), StableRep achieved 57.6 percent accuracy, while the best competing model, CLIP pretrained on generated images, scored 53.5 percent.\nWhy it matters: The fact that text-to-image generators can produce images of similar things that are quite different in appearance makes them a powerful resource for training vision models. And they provide a practically unlimited source of such images!\nWe're thinking: Different foundation models understand different aspects of the world. It’s exciting that a large diffusion model, which is good at generating images, can be used to train a large vision transformer, which is good at analyzing images!\n\n\n",
  "image_filename": "stablerep-a-method-that-trains-vision-transformers-on-images-generated-by-stable-diffusion.gif"
}
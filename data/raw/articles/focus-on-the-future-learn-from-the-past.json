{
  "title": "Focus on the Future, Learn From the Past",
  "url": "https://www.deeplearning.ai/the-batch/focus-on-the-future-learn-from-the-past/",
  "text": "Dear friends,\nI‚Äôm thrilled that former students and postdocs of mine won both of this year‚Äôs NeurIPS Test of Time Paper Awards. This award recognizes papers published 10 years ago that have significantly shaped the research field. The recipients included Ian Goodfellow (who, as an undergraduate, built my first GPU server for deep learning in his dorm room) and his collaborators for their work on generative adversarial networks, and my former postdoc Ilya Sutskever and PhD student Quoc Le (with Oriol Vinyals) for their work on sequence-to-sequence learning. Congratulations to all these winners!\nBy nature, I tend to focus on the future rather than the past. Steve Jobs famously declined to build a corporate museum, instead donating Apple's archives to Stanford University, because he wanted to keep the company forward-looking. Jeff Bezos encourages teams to approach every day as if it were ‚ÄúDay 1,‚Äù a mindset that emphasizes staying in the early, innovative stage of a company or industry. These philosophies resonate with me.\nBut taking a brief look at the past can help us reflect on lessons for the future. One takeaway from looking at what worked 10 to 15 years ago is that many of the teams I led bet heavily on scaling to drive AI progress ‚Äî a bet that laid a foundation to build larger and larger AI systems. At the time, the idea of scaling up neural networks was controversial, and I was on the fringe. I recall distinctly that, around ¬†2008, Yoshua Bengio advised me not to bet on scaling and to focus on inventing algorithms instead!\nA lesson I carry from that time is to not worry about what others think, but follow your convictions, especially if you have data to support your beliefs. Small-scale experiments performed by my Stanford group convinced me that scaling up neural networks would drive significant progress, and that‚Äôs why I was willing to ignore the skeptics. The diagram below, generated by Adam Coates and Honglak Lee, is the one that most firmed up my beliefs at that time. It shows that, for a range of models, the larger we scaled them, the better they perform. I remember presenting it at CIFAR 2010 , and if I had to pick a single reason why I pushed through to start Google Brain and set as the team‚Äôs #1 goal to scale up deep learning algorithms, it is this diagram!\nI also remember presenting at NeurIPS in 2008 our work on using GPUs to scale up training neural networks. (By the way, one measure of success in academia is when your work becomes sufficiently widely accepted that no one cites it anymore. I‚Äôm quite pleased the idea that GPUs should be used for AI ‚Äî which was controversial back then ‚Äî is now such a widely accepted ‚Äúfact‚Äù that no one bothers to cite early papers that pushed for it.üòÉ)\nWhen I started Google Brain, the thesis was simple: I wanted to use the company‚Äôs ¬†huge computing capability to scale up deep learning. Shortly afterward, I built Stanford‚Äôs first supercomputer for deep learning using GPUs, since I could move faster at Stanford than within a large company. A few years later, my team at Baidu showed that as you scale up a model, its performance improves linearly on a log-log scale, which was a precursor to OpenAI‚Äôs scaling laws.\nAs I look to the future, I‚Äôm sure there are ideas that many people are skeptical about today, but will prove to be accurate. Scaling up AI models turned out to be useful for many teams, and it continues to be exciting, but now I‚Äôm even more excited by upcoming ideas that will prove to be even more valuable in the future.\nThis past year, I spent a lot of time encouraging teams to build applications with agentic AI and worked to share best practices. I have a few hypotheses for additional technologies that will be important next year. I plan to spend the winter holiday playing with a few of them, and I will have more to share next year. But if you have an idea that you have conviction on, so long as you can do so responsibly, I encourage you to pursue it!\nKeep learning,\nAndrew\n\n\n",
  "image_filename": "focus-on-the-future-learn-from-the-past.png"
}
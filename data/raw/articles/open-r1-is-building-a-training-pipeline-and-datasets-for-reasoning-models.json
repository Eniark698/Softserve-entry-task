{
  "title": "Open-R1 is building a training pipeline and datasets for reasoning models",
  "url": "https://www.deeplearning.ai/the-batch/open-r1-is-building-a-training-pipeline-and-datasets-for-reasoning-models/",
  "text": "Twice a week, Data Points brings you the latest AI news, tools, models, and research in brief. In today’s edition, you’ll find:\nWiz finds DeepSeek’s unprotected user database\nOpen weights model Mistral Small gets an update\nJanus-Pro is DeepSeek’s top multimodal model\nYoshua Bengio’s team releases long-awaited AI safety report\nBut first:\nOpen source project aims to replicate R1 reasoning model\nA new initiative called Open-R1 seeks to reconstruct DeepSeek’s recently released R1 reasoning model, which rivaled OpenAI’s o1 in performance. The project would replicate DeepSeek’s training pipeline, including data curation and reinforcement learning techniques, to create open source reasoning models, with an initial focus on mathematics. By sharing reproducible insights and training recipes, Open-R1 hopes to advance research in AI reasoning capabilities beyond math to areas like science and medicine. ( Hugging Face )\nOpenAI makes important updates to GPT-4o and Canvas\nCanvas, a ChatGPT feature that allows users to collaborate with AI to create a document or code, now works with OpenAI’s advanced o1 model and can render HTML or React code in the browser. OpenAI also refreshed GPT-4o, updating its knowledge cutoff from November 2023 to June 2024 and improving its performance on math, image understanding, and general reasoning. These updates give users access to more recent information and more powerful tools for development in the API, and make ChatGPT’s Canvas more competitive with Claude’s similar Artifacts feature. ( OpenAI and X )\nDeepSeek exposes sensitive data due to security oversight\nCybersecurity firm Wiz uncovered a major security lapse at Chinese AI startup DeepSeek, finding over a million lines of sensitive data exposed on the open internet through an unsecured ClickHouse database. The exposed information included software keys, user chat logs, API secrets, and backend details, allowing potential attackers full control over database operations and access to internal data. Wiz researchers argue that the rapid growth of companies like DeepSeek shows the critical need for robust security measures to protect user data and maintain trust. ( Wiz )\nMistral unveils open AI model that rivals larger competitors\nMistral released Mistral Small 3, a 24 billion parameter language model that matches the performance of models three times its size while offering lower latency. The model, available under the Apache 2.0 license, excels in tasks requiring robust language understanding and instruction following with very fast response times. This release renews Mistral’s commitment to open AI development in the run-up to the company’s expected IPO, as the company promises that more future releases will be under the Apache 2.0 license rather than proprietary ones. ( Mistral )\nDeepSeek’s new vision-language model also generates images\nDeepSeek researchers developed Janus-Pro, an upgraded suite of vision-language models that can understand and generate images and text. Janus-Pro improves on its predecessor by using smarter training methods, more diverse datasets, and larger neural networks. On benchmark tests, Janus-Pro outperformed both specialized and generalist systems like DALL·E 3, Stable Diffusion, and Qwen-VL at tasks like analyzing images and generating pictures from text descriptions. Available in one billion and seven billion parameter versions, Janus-Pro is another strong offering from DeepSeek, showing how strategic improvements in AI training and architecture can lead to significant performance gains. ( GitHub )\nInternational report warns of extreme risks from advanced AI\nA new report backed by 30 countries outlines potential dangers from advanced AI systems, including job displacement, terrorism, and loss of human control. The report, led by AI scientist Yoshua Bengio, aims to guide policymakers in creating safeguards for rapidly advancing AI technology. This synthesis of existing research follows last year’s AI summit in the UK and comes ahead of a similarly major international summit in Paris. ( Gov.UK and the Associated Press )\nStill want to know more about what matters in AI right now?\nRead this week’s issue of The Batch for in-depth analysis of news and research.\nThis week, Andrew Ng reflected on DeepSeek’s impact, highlighting China’s rapid progress in generative AI, the growing influence of open models in the AI supply chain, and the importance of algorithmic innovation beyond just scaling up.\n“If the U.S. continues to stymie open source, China will come to dominate this part of the supply chain and many businesses will end up using models that reflect China’s values much more than America’s.”\nRead Andrew’s full letter here .\nOther top AI news and research stories we covered in depth: how DeepSeek-R1 and Kimi k1.5 leveraged reinforcement learning to train reasoning models, pushing the boundaries of AI capabilities; OpenAI introduced Operator , an AI agent designed to automate online tasks; The White House made a bold policy shift , rolling back AI regulations and emphasizing the need for U.S. leadership in the global market; and Cohere researchers proposed active inheritance, a novel fine-tuning approach that lets model-makers automatically select better synthetic data.\nSubscribe to Data Points\n\n\n",
  "image_filename": "open-r1-is-building-a-training-pipeline-and-datasets-for-reasoning-models.jpg"
}
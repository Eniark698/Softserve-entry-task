{
  "title": "A new technique to build simple but powerful reasoning models",
  "url": "https://www.deeplearning.ai/the-batch/a-new-technique-to-build-simple-but-powerful-reasoning-models/",
  "text": "Twice a week, Data Points brings you the latest AI news, tools, models, and research in brief. In today’s edition, you’ll find:\nClaude’s new method to thwart universal jailbreaks\nDeepMind team shares recipes for model scaling\nCopilot adds agent mode, previews more autonomous tools\nπ0 robotics foundation models are now open source\nBut first:\nOpen reasoning model fine-tuned using just 1,000 examples\nStanford researchers created a new AI reasoning model called s1-32B by fine-tuning Qwen2.5-32B-Instruct on just 1,000 carefully selected examples distilled from Google’s Gemini 2.0 Flash Thinking. The resulting s1-32B model matches or exceeds the performance of more complex closed models on challenging math and science benchmarks while being fully open source (model, data, and code). The researchers introduced a simple “budget forcing” technique that either ends the model’s thinking process when it exceeds a maximum token limit or extends it by appending “Wait” to the current reasoning trace when the model tries to conclude too early. This allows s1-32B to improve its reasoning as more compute is applied at test time, similar to capabilities seen in proprietary models but achieved with a much simpler approach. ( arXiv and GitHub )\nGoogle expands Gemini lineup with new capabilities\nGoogle released several updates to its Gemini 2.0 AI model family, including a generally available version of Gemini 2.0 Flash and an experimental version of Gemini 2.0 Pro. The company also introduced Gemini 2.0 Flash-Lite, a cost-efficient model with improved quality over its predecessor, and made 2.0 Flash Thinking Experimental available to Gemini app users. All of the Gemini 2.0 models can accept text and image inputs and return text outputs. The new Gemini 2.0 Flash costs slightly more than its predecessor, but Gemini 2.0 Flash-Lite is priced the same as Gemini 1.5 Flash. ( Google )\nAnthropic develops robust defense against universal jailbreaks\nAnthropic’s new Constitutional Classifiers system successfully defended against thousands of hours of human attempts to jailbreak its Claude models. The method reduced jailbreak success rates from 86% to 4.4% in automated tests, with minimal increases in refusal rates and compute costs. The system works by training input and output classifiers on synthetically generated data based on a “constitution” of allowed and disallowed content, enabling it to detect and block potentially harmful inputs and outputs. Anthropic is hosting a live demo and offering rewards up to $20,000 for successful jailbreaks to further test and improve the system’s robustness. ( Anthropic and arXiv )\nGitHub Copilot introduces agent mode and expands AI capabilities\nGitHub unveiled new features for its Copilot AI assistant, including an agent mode that can autonomously iterate on code and fix errors. The company also announced the general availability of Copilot Edits in Visual Studio Code, which allows developers to make multi-file changes using natural language commands. GitHub teased Project Padawan, an upcoming autonomous software engineering agent that can handle entire issues and pull requests, which could change how development teams manage routine tasks. ( GitHub )\nRobotics company releases open source foundation model\nPhysical Intelligence made the code and weights for π0, their general-purpose vision-language-action model, available for download under an Apache 2.0 license (along with π0-FAST, which uses a different tokenizer). The model can be fine-tuned for various tasks across different robot types, with the company providing pre-trained checkpoints, example code, and fine-tuning instructions. π0 is particularly good at everyday tasks, like laundry-folding, and following instructions in natural language. The open source release aims to accelerate development of physical AI systems that can interact with and understand the world intuitively. ( Physical Intelligence and Hugging Face )\nNew online book, “How to Scale Your Model,” demystifies training\nGoogle DeepMind researchers published a comprehensive guide on scaling language models using tensor processing units (TPUs). The book covers TPU architecture, efficient parallelization techniques, and practical tutorials for training and serving massive language models like Gemini 2.0 and Llama 3. This resource aims to help AI developers optimize model performance, estimate training costs, and make informed decisions about hardware utilization as language models continue growing in size and complexity. ( GitHub )\nStill want to know more about what matters in AI right now?\nRead this week’s issue of The Batch for in-depth analysis of news and research.\nThis week, Andrew Ng explores how AI is enabling a new generation of ‘10x professionals’ across various industries, not just in engineering, by transforming workflows and amplifying impact within and across teams.\n“A ‘10x engineer’—a widely accepted concept in tech—purportedly has 10 times the impact of the average engineer. But we don’t seem to have 10x marketers, 10x recruiters, or 10x financial analysts. As more jobs become AI-enabled, I think this will change, and there will be a lot more ‘10x professionals.’”\nRead Andrew’s full letter here .\nOther top AI news and research stories we covered in depth: OpenAI launched o3-mini , a faster and more cost-effective reasoning model excelling in coding, math, and science; UI-TARS demonstrated strong performance in computer use benchmarks, demonstrating its ability to interact with desktop and mobile interfaces; Google’s update to Gemini 2.0 Flash Thinking outperformed DeepSeek-R1 on key benchmarks; and Moshi, an open-source alternative to OpenAI’s Realtime API , showcased its always-on speech-to-speech interactions.\nSubscribe to Data Points\n\n\n",
  "image_filename": "a-new-technique-to-build-simple-but-powerful-reasoning-models.jpg"
}